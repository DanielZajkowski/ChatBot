{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a ChatBot with Deep NLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/danielzajkowski/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielzajkowski/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "import tensorflow_addons as tfa\n",
    "import tf_slim as tfs\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conversations = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary that maps each line and its id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a list of all of the conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(','))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting separately the questions and the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        questions.append(id2line[conversation[i]])\n",
    "        answers.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing a first cleaning of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}+=~|.?,]\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_answers = []\n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary that maps each word to its number of occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count = {}\n",
    "for question in clean_questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1\n",
    "\n",
    "for answer in clean_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating two dictionaries that map the questions words and the answers words to a unique integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "\n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the last tokens to these two dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "for token in tokens:\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the inverse dictionary of the answerswords2int dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "answersints2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the End Of String token to the end of every answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clean_answers)):\n",
    "    clean_answers[i] += ' <EOS>'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating all the questions and the answers into integers and Replacing all the words that were filtered out by < OUT >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_into_int = []\n",
    "for question in clean_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    questions_into_int.append(ints)\n",
    "\n",
    "answers_into_int = []\n",
    "for answer in clean_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int['<OUT>'])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    answers_into_int.append(ints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting questions and answers by the lenght of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_clean_questions = []\n",
    "sorted_clean_answers = []\n",
    "for length in range (1, 25 + 1):\n",
    "    for i in enumerate(questions_into_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_clean_questions.append(questions_into_int[i[0]])\n",
    "            sorted_clean_answers.append(answers_into_int[i[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Seq2Seq Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating placeholder for the inputs and the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs = tf.compat.v1.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.compat.v1.placeholder(tf.int32, [None, None], name='target')\n",
    "    lr = tf.compat.v1.placeholder(tf.float32, [None, None], name='learning_rate')\n",
    "    keep_prob = tf.compat.v1.placeholder(tf.float32, name='keep_prob')\n",
    "    return inputs, targets, lr, keep_prob"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    left_side = tf.fill([batch_size, 1], word2int['<SOS>'])\n",
    "    right_side = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
    "    preprocess_targets = tf.concat([left_side, right_side], 1)\n",
    "    return preprocess_targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Encoder RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length):\n",
    "    lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "    encoder_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    encoder_output, encoder_state = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell, cell_bw = encoder_cell, sequence_length = sequence_length, inputs = rnn_inputs, dtype = tf.float32)\n",
    "    return encoder_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tfa.seq2seq.AttentionWrapper(attention_states, attention_option=\"bahdanau\", num_units=decoder_cell.output_size)\n",
    "    training_decoder_function = tfa.seq2seq.BasicDecoder(encoder_state[0], attention_keys, attention_values, attention_score_function, attention_construct_function, name=\"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tfa.seq2seq.dynamic_decode(decoder_cell, training_decoder_function, decoder_embedded_input, sequence_length, scope=decoding_scope)\n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding the test/validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix_input, sos_id, eos_id, maximum_length, num_words, decoding_scope, output_function, batch_size):\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tfa.seq2seq.AttentionWrapper(attention_states, attention_option=\"bahdanau\", num_units=decoder_cell.output_size)\n",
    "    test_decoder_function = tfa.seq2seq.BasicDecoder(output_function, encoder_state[0], attention_keys, attention_values, attention_score_function, attention_construct_function, decoder_embeddings_matrix_input, sos_id, eos_id, maximum_length, num_words, name=\"attn_dec_inf\")\n",
    "    test_predictions, _, _ = tfa.seq2seq.dynamic_decode(decoder_cell, test_decoder_function, scope=decoding_scope)\n",
    "    return test_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, num_words, sequence_length, rnn_size, num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.compat.v1.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(rnn_size)\n",
    "        lstm_dropout = tf.compat.v1.nn.rnn_cell.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n",
    "        decoder_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        weights = tf.compat.v1.truncated_normal_initializer(stddev = 0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        output_function = lambda x: tfs.fully_connected(x, num_words, None, scope = decoding_scope, weights_initializer = weights, biases_initializer = biases)\n",
    "        training_predictions = decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, output_function, keep_prob, batch_size)\n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predictions = decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, word2int['<SOS>'], word2int['<EOS>'], sequence_length - 1, num_words, decoding_scope, output_function, keep_prob, batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs -- question ask to the chatbot\n",
    "# target -- answer\n",
    "# answers_num_words -- total number of words in all answers\n",
    "# encoder_embedding_size -- number of dimesnison in the embbeding matrix\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n",
    "    encoder_embedded_input = tf.keras.layers.Embedding(answers_num_words + 1, encoder_embedding_size, embeddings_initializer = tf.random_uniform_initializer(0,1))\n",
    "    encoder_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    preprocesed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    decoder_embeddings_matrix = tf.Variable(tf.compat.v1.random_uniform([questions_num_words + 1, decoder_embedding_size],0, 1))\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocesed_targets)\n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, questions_num_words, sequence_length, rnn_size, num_layers, questionswords2int, keep_prob, batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Seq2Seq Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 512 \n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.InteractiveSession at 0x105b23eb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, lr, keep_prob = model_inputs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = tf.compat.v1.placeholder_with_default(25, None, name = 'sequence_length')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the shape of the inputs tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the training and test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_k/56ln05tx4vs6_gx2ht0p_pzc0000gn/T/ipykernel_91566/677993718.py:2: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  lstm = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(rnn_size)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert elements of <keras.layers.core.embedding.Embedding object at 0x2d7a03760> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_predictions, test_predictions \u001b[39m=\u001b[39m seq2seq_model(tf\u001b[39m.\u001b[39;49mreverse(inputs, [\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), targets, keep_prob, batch_size, sequence_length, \u001b[39mlen\u001b[39;49m(answerswords2int), \u001b[39mlen\u001b[39;49m(questionswords2int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, questionswords2int)\n",
      "Cell \u001b[0;32mIn[51], line 7\u001b[0m, in \u001b[0;36mseq2seq_model\u001b[0;34m(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseq2seq_model\u001b[39m(inputs, targets, keep_prob, batch_size, sequence_length, answers_num_words, questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, num_layers, questionswords2int):\n\u001b[1;32m      6\u001b[0m     encoder_embedded_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mEmbedding(answers_num_words \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, encoder_embedding_size, embeddings_initializer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mrandom_uniform_initializer(\u001b[39m0\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m     encoder_state \u001b[39m=\u001b[39m encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n\u001b[1;32m      8\u001b[0m     preprocesed_targets \u001b[39m=\u001b[39m preprocess_targets(targets, questionswords2int, batch_size)\n\u001b[1;32m      9\u001b[0m     decoder_embeddings_matrix \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mVariable(tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mrandom_uniform([questions_num_words \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, decoder_embedding_size],\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "Cell \u001b[0;32mIn[46], line 5\u001b[0m, in \u001b[0;36mencoder_rnn\u001b[0;34m(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m lstm_dropout \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrnn_cell\u001b[39m.\u001b[39mDropoutWrapper(lstm, input_keep_prob \u001b[39m=\u001b[39m keep_prob)\n\u001b[1;32m      4\u001b[0m encoder_cell \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrnn_cell\u001b[39m.\u001b[39mMultiRNNCell([lstm_dropout] \u001b[39m*\u001b[39m num_layers)\n\u001b[0;32m----> 5\u001b[0m encoder_output, encoder_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mbidirectional_dynamic_rnn(cell_fw \u001b[39m=\u001b[39;49m encoder_cell, cell_bw \u001b[39m=\u001b[39;49m encoder_cell, sequence_length \u001b[39m=\u001b[39;49m sequence_length, inputs \u001b[39m=\u001b[39;49m rnn_inputs, dtype \u001b[39m=\u001b[39;49m tf\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m      6\u001b[0m \u001b[39mreturn\u001b[39;00m encoder_state\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tensorflow/python/util/deprecation.py:371\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m         _PRINTED_WARNING[\u001b[39mcls\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    365\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) is deprecated and will be removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    366\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m, _call_location(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[1;32m    370\u001b[0m         instructions)\n\u001b[0;32m--> 371\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tensorflow/python/framework/tensor_util.py:609\u001b[0m, in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    607\u001b[0m   str_values \u001b[39m=\u001b[39m [compat\u001b[39m.\u001b[39mas_bytes(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m proto_values]\n\u001b[1;32m    608\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m--> 609\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to convert elements of \u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m}\u001b[39;00m\u001b[39m to Tensor. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mConsider casting elements to a supported type. See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mhttps://www.tensorflow.org/api_docs/python/tf/dtypes \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mfor supported TF dtypes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    613\u001b[0m tensor_proto\u001b[39m.\u001b[39mstring_val\u001b[39m.\u001b[39mextend(str_values)\n\u001b[1;32m    614\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_proto\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert elements of <keras.layers.core.embedding.Embedding object at 0x2d7a03760> to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes."
     ]
    }
   ],
   "source": [
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]), targets, keep_prob, batch_size, sequence_length, len(answerswords2int), len(questionswords2int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, questionswords2int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
